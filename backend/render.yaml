services:
  # Backend API service
  - type: web
    name: pixel-cortex-backend
    runtime: python
    region: oregon # optional
    plan: free
    buildCommand: "pip install -r requirements.render.txt"
    startCommand: "uvicorn run_basic:app --host 0.0.0.0 --port $PORT"
    envVars:
      - key: DATABASE_URL
        fromDatabase:
          name: pixel-cortex-db
          property: connectionString
      - key: SECRET_KEY
        generateValue: true
      - key: ALGORITHM
        value: HS256
      - key: ACCESS_TOKEN_EXPIRE_MINUTES
        value: 30
      - key: AUTH_DISABLED
        value: true # Set to false for production
      - key: DEMO_MODE
        value: true # Set to false for production
      - key: POLICIES_DIR
        value: ./policies
      - key: USE_VOSK
        value: true
      - key: VOSK_MODEL_URL
        value: https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
      - key: VOSK_MODEL_PATH
        value: /tmp/vosk-model
      
      # LLM Configuration (choose one):
      
      # Option 1: Use Together.ai API (free tier available)
      # Get API key from: https://api.together.xyz/
      - key: USE_LLM_API
        value: false  # Set to true to use API
      - key: LLM_API_KEY
        value: ""  # Add your Together.ai API key here
      - key: LLM_API_URL
        value: https://api.together.xyz/inference
      
      # Option 2: Connect to external Ollama server
      # Deploy Ollama on RunPod/Vast.ai with GPU
      - key: OLLAMA_HOST
        value: ""  # Add your Ollama server URL here

databases:
  # PostgreSQL database
  - name: pixel-cortex-db
    databaseName: pixel_cortex
    plan: free # Use 'starter' for production
