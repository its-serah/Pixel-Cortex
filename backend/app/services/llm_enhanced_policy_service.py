"""
LLM-Enhanced Policy Retrieval Service

Combines the existing KG-RAG system with local LLM reasoning for intelligent
policy analysis, contextual understanding, and enhanced decision support.
"""

import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from sqlalchemy.orm import Session

from app.models.schemas import (
    PolicyCitation, KGEnhancedPolicyCitation, GraphHop,
    TriageExplanation, KGEnhancedTriageExplanation
)
from app.services.policy_retriever import PolicyRetriever
from app.services.knowledge_graph_query import KnowledgeGraphQueryService
from app.services.local_llm_service import local_llm_service
from app.services.prompt_engineering_service import prompt_engineering_service, PromptTemplate, GuardrailLevel
from app.services.conversation_memory_service import conversation_memory_service


logger = logging.getLogger(__name__)


class LLMEnhancedPolicyService:
    """Service combining KG-RAG with local LLM for intelligent policy analysis"""
    
    def __init__(self):
        self.policy_retriever = PolicyRetriever()
        self.kg_query_service = KnowledgeGraphQueryService()
    
    def intelligent_policy_search(
        self,
        query: str,
        db: Session,
        user_id: Optional[int] = None,
        k: int = 5,
        use_conversation_context: bool = True,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Perform intelligent policy search using LLM reasoning and KG enhancement
        
        Args:
            query: User query
            db: Database session
            user_id: User ID for context
            k: Number of results to return
            use_conversation_context: Whether to use conversation history
            session_id: Current session ID
            
        Returns:
            Enhanced search results with LLM reasoning
        """
        
        start_time = datetime.now()
        
        # Step 1: Get conversation context if enabled
        conversation_context = {}
        if use_conversation_context and user_id:
            conversation_context = conversation_memory_service.get_contextual_memory(
                db, query, user_id, session_id
            )
        
        # Step 2: Enhance query with LLM understanding
        enhanced_query_result = self._enhance_query_with_llm(
            query, conversation_context
        )\n        \n        if not enhanced_query_result[\"success\"]:\n            # Fall back to original query\n            enhanced_query = query\n            query_analysis = {\"original_query\": query, \"enhancement_failed\": True}\n        else:\n            enhanced_query = enhanced_query_result[\"response\"].get(\"enhanced_query\", query)\n            query_analysis = enhanced_query_result[\"response\"]\n        \n        # Step 3: Perform KG-Enhanced RAG retrieval\n        enhanced_citations, graph_hops, kg_metadata = self.policy_retriever.kg_enhanced_retrieve(\n            query=enhanced_query,\n            k=k,\n            enable_kg=True,\n            db=db\n        )\n        \n        # Step 4: Analyze policies with LLM for deeper understanding\n        policy_analysis = self._analyze_policies_with_llm(\n            original_query=query,\n            enhanced_query=enhanced_query,\n            citations=enhanced_citations,\n            conversation_context=conversation_context\n        )\n        \n        # Step 5: Generate comprehensive reasoning\n        reasoning = self._generate_comprehensive_reasoning(\n            query=query,\n            citations=enhanced_citations,\n            graph_hops=graph_hops,\n            policy_analysis=policy_analysis,\n            context=conversation_context\n        )\n        \n        processing_time = (datetime.now() - start_time).total_seconds() * 1000\n        \n        return {\n            \"original_query\": query,\n            \"enhanced_query\": enhanced_query,\n            \"query_analysis\": query_analysis,\n            \"enhanced_citations\": enhanced_citations,\n            \"graph_hops\": graph_hops,\n            \"policy_analysis\": policy_analysis,\n            \"comprehensive_reasoning\": reasoning,\n            \"conversation_context_used\": use_conversation_context,\n            \"kg_metadata\": kg_metadata,\n            \"processing_time_ms\": processing_time,\n            \"total_results\": len(enhanced_citations)\n        }\n    \n    def _enhance_query_with_llm(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Enhance user query with LLM understanding and context\"\"\"\n        \n        # Build context summary\n        context_summary = \"\"\n        if context.get(\"conversation_history\"):\n            recent_topics = [h[\"user_message\"][:50] for h in context[\"conversation_history\"][-2:]]\n            context_summary += f\"Recent topics: {', '.join(recent_topics)}. \"\n        \n        if context.get(\"user_patterns\", {}).get(\"common_topics\"):\n            common_topics = context[\"user_patterns\"][\"common_topics\"][:3]\n            context_summary += f\"User often asks about: {', '.join(common_topics)}. \"\n        \n        query_enhancement_prompt = f\"\"\"Analyze and enhance this IT support query for better policy search:\n\nOriginal Query: \"{query}\"\nContext: {context_summary if context_summary else \"No previous context\"}\n\nEnhance the query by:\n1. Identifying key IT concepts and technical terms\n2. Adding relevant synonyms and alternative phrasings\n3. Clarifying the underlying issue or need\n4. Suggesting related concepts to search for\n\nReturn JSON:\n{{\n    \"enhanced_query\": \"improved search query\",\n    \"key_concepts\": [\"VPN\", \"MFA\"],\n    \"search_intent\": \"troubleshooting|information|approval|access\",\n    \"related_terms\": [\"synonym1\", \"synonym2\"],\n    \"confidence\": 0.8\n}}\n\nJSON Response:\"\"\"\n        \n        return prompt_engineering_service.generate_safe_response(\n            PromptTemplate.POLICY_ANALYSIS,\n            {\"policy_text\": query_enhancement_prompt, \"user_request\": query},\n            GuardrailLevel.MODERATE\n        )\n    \n    def _analyze_policies_with_llm(\n        self,\n        original_query: str,\n        enhanced_query: str,\n        citations: List[KGEnhancedPolicyCitation],\n        conversation_context: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Analyze retrieved policies using LLM for deeper understanding\"\"\"\n        \n        if not citations:\n            return {\"analysis\": \"No policies found for analysis\", \"relevance_scores\": []}\n        \n        # Prepare policy texts for analysis\n        policy_texts = []\n        for citation in citations[:3]:  # Analyze top 3 most relevant\n            policy_text = f\"Policy: {citation.document_title}\\nContent: {citation.chunk_content}\"\n            policy_texts.append(policy_text)\n        \n        combined_policies = \"\\n\\n\".join(policy_texts)\n        \n        policy_analysis_prompt = f\"\"\"Analyze these IT policies in relation to the user's query:\n\nUser Query: \"{original_query}\"\nEnhanced Query: \"{enhanced_query}\"\n\nPolicies:\n{combined_policies}\n\nProvide analysis:\n1. Overall applicability to the query (high/medium/low)\n2. Key policy requirements or restrictions\n3. Potential conflicts or considerations\n4. Recommended action based on policies\n5. Confidence in interpretation (0.0-1.0)\n\nReturn JSON:\n{{\n    \"applicability\": \"high\",\n    \"key_requirements\": [\"MFA required\", \"Manager approval needed\"],\n    \"considerations\": [\"Security implications\", \"Compliance requirements\"],\n    \"recommended_action\": \"approved_with_conditions\",\n    \"confidence\": 0.8,\n    \"reasoning\": \"detailed explanation\"\n}}\n\nJSON Response:\"\"\"\n        \n        analysis_result = prompt_engineering_service.generate_safe_response(\n            PromptTemplate.POLICY_ANALYSIS,\n            {\"policy_text\": combined_policies, \"user_request\": original_query},\n            GuardrailLevel.MODERATE\n        )\n        \n        if analysis_result[\"success\"]:\n            return analysis_result[\"response\"]\n        else:\n            return {\n                \"analysis\": \"Policy analysis failed\",\n                \"error\": analysis_result.get(\"error\", \"Unknown error\"),\n                \"confidence\": 0.3\n            }\n    \n    def _generate_comprehensive_reasoning(\n        self,\n        query: str,\n        citations: List[KGEnhancedPolicyCitation],\n        graph_hops: List[GraphHop],\n        policy_analysis: Dict[str, Any],\n        context: Dict[str, Any]\n    ) -> str:\n        \"\"\"Generate comprehensive reasoning explanation using LLM\"\"\"\n        \n        # Build reasoning context\n        reasoning_context = f\"\"\"User Query: {query}\n\nPolicy Search Results: {len(citations)} policies found\nKnowledge Graph Hops: {len(graph_hops)} concept relationships discovered\nPolicy Analysis: {policy_analysis.get('reasoning', 'Analysis not available')}\n\nContext Summary: {context.get('context_summary', 'No previous context')}\"\"\"\n        \n        reasoning_prompt = f\"\"\"Generate a comprehensive explanation for this IT support analysis:\n\n{reasoning_context}\n\nProvide a clear, structured explanation that:\n1. Summarizes the user's request\n2. Explains how policies apply to the situation\n3. Describes any relevant knowledge graph connections\n4. Gives actionable next steps\n5. Notes any important considerations or warnings\n\nKeep the explanation professional but accessible (max 300 words):\"\"\"\n        \n        reasoning_result = local_llm_service.generate_response(\n            reasoning_prompt,\n            max_tokens=512,\n            temperature=0.7,\n            context={\"task\": \"reasoning_generation\"}\n        )\n        \n        if reasoning_result.get(\"response\"):\n            return reasoning_result[\"response\"]\n        else:\n            return f\"\"\"Based on your query \"{query}\", I found {len(citations)} relevant policies. \nThe knowledge graph analysis discovered {len(graph_hops)} related concepts. \n{policy_analysis.get('reasoning', 'Please review the policy citations for specific guidance.')}\"\"\"\n    \n    def contextual_policy_guidance(\n        self,\n        user_query: str,\n        ticket_context: Optional[Dict[str, Any]],\n        db: Session,\n        user_id: Optional[int] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Provide contextual policy guidance for specific IT support scenarios\"\"\"\n        \n        # Get user patterns and history\n        user_context = {}\n        if user_id:\n            user_context = conversation_memory_service.get_user_interaction_patterns(\n                db, user_id, days_back=30\n            )\n        \n        # Perform intelligent search\n        search_results = self.intelligent_policy_search(\n            query=user_query,\n            db=db,\n            user_id=user_id,\n            k=5,\n            use_conversation_context=True\n        )\n        \n        # Generate specific guidance using LLM\n        guidance_prompt = f\"\"\"Provide specific IT support guidance for this scenario:\n\nUser Query: \"{user_query}\"\nUser's Common Topics: {user_context.get('common_topics', [])}\nTicket Context: {ticket_context.get('category', 'unknown')} - {ticket_context.get('priority', 'unknown')}\n\nRelevant Policies Found: {len(search_results['enhanced_citations'])}\nKnowledge Graph Insights: {len(search_results['graph_hops'])} relationships\n\nPolicy Analysis: {search_results['policy_analysis'].get('reasoning', 'No analysis available')}\n\nProvide actionable guidance:\n1. **Immediate Actions**: What can be done right away\n2. **Policy Requirements**: What policies apply and their requirements\n3. **Approval Process**: If approvals are needed and from whom\n4. **Timeline**: Expected timeline for resolution\n5. **Follow-up**: Any follow-up actions or monitoring needed\n\nKeep guidance practical and specific to this user's context.\n\nGuidance:\"\"\"\n        \n        guidance_result = local_llm_service.generate_response(\n            guidance_prompt,\n            max_tokens=768,\n            temperature=0.6,\n            context={\"task\": \"policy_guidance\"}\n        )\n        \n        return {\n            \"guidance\": guidance_result[\"response\"],\n            \"search_results\": search_results,\n            \"user_context\": user_context,\n            \"ticket_context\": ticket_context,\n            \"confidence\": search_results[\"policy_analysis\"].get(\"confidence\", 0.7),\n            \"processing_time_ms\": search_results[\"processing_time_ms\"] + guidance_result.get(\"inference_time_ms\", 0)\n        }\n    \n    def analyze_policy_compliance(\n        self,\n        request_description: str,\n        relevant_policies: List[Dict[str, Any]],\n        db: Session\n    ) -> Dict[str, Any]:\n        \"\"\"Analyze compliance of a request against relevant policies using LLM\"\"\"\n        \n        # Prepare policy context\n        policy_context = \"\"\n        for i, policy in enumerate(relevant_policies[:3]):\n            policy_context += f\"\\nPolicy {i+1}: {policy.get('title', 'Unknown')}\\n{policy.get('content', '')[:500]}...\"\n        \n        compliance_prompt = f\"\"\"Analyze the compliance of this IT request against company policies:\n\nRequest: \"{request_description}\"\n\nRelevant Policies:{policy_context}\n\nAnalyze:\n1. Compliance status: compliant|non_compliant|requires_review|conditional\n2. Specific policy violations or requirements\n3. Required approvals or conditions\n4. Risk assessment (low/medium/high)\n5. Recommended decision (approve/deny/conditional_approval)\n\nReturn JSON:\n{{\n    \"compliance_status\": \"compliant\",\n    \"violations\": [],\n    \"requirements\": [\"MFA verification\", \"Manager approval\"],\n    \"risk_level\": \"medium\",\n    \"recommended_decision\": \"conditional_approval\",\n    \"reasoning\": \"detailed explanation\",\n    \"confidence\": 0.8\n}}\n\nJSON Response:\"\"\"\n        \n        compliance_result = prompt_engineering_service.generate_safe_response(\n            PromptTemplate.POLICY_ANALYSIS,\n            {\"policy_text\": policy_context, \"user_request\": request_description},\n            GuardrailLevel.MODERATE\n        )\n        \n        if compliance_result[\"success\"]:\n            return compliance_result[\"response\"]\n        else:\n            return {\n                \"compliance_status\": \"requires_review\",\n                \"error\": compliance_result.get(\"error\", \"Analysis failed\"),\n                \"recommended_decision\": \"manual_review\",\n                \"confidence\": 0.3\n            }\n    \n    def generate_policy_recommendations(\n        self,\n        scenario: str,\n        current_policies: List[Dict[str, Any]],\n        db: Session\n    ) -> Dict[str, Any]:\n        \"\"\"Generate policy recommendations for IT support scenarios\"\"\"\n        \n        # Analyze current policy coverage\n        coverage_analysis = self._analyze_policy_coverage(scenario, current_policies)\n        \n        # Generate recommendations using LLM\n        recommendation_prompt = f\"\"\"Analyze this IT support scenario and provide policy recommendations:\n\nScenario: \"{scenario}\"\n\nCurrent Policy Coverage: {coverage_analysis}\n\nExisting Policies: {len(current_policies)} policies available\n\nProvide recommendations:\n1. **Policy Gaps**: Areas not covered by current policies\n2. **Enhancement Opportunities**: How existing policies could be improved\n3. **New Policy Needs**: Specific new policies that should be created\n4. **Implementation Priority**: Priority order for policy updates (high/medium/low)\n5. **Risk Mitigation**: How recommendations address current risks\n\nFocus on practical, implementable recommendations for IT operations.\n\nRecommendations:\"\"\"\n        \n        recommendation_result = local_llm_service.generate_response(\n            recommendation_prompt,\n            max_tokens=768,\n            temperature=0.6,\n            context={\"task\": \"policy_recommendations\"}\n        )\n        \n        return {\n            \"scenario\": scenario,\n            \"recommendations\": recommendation_result[\"response\"],\n            \"coverage_analysis\": coverage_analysis,\n            \"current_policy_count\": len(current_policies),\n            \"confidence\": 0.7,\n            \"generated_at\": datetime.now().isoformat()\n        }\n    \n    def _analyze_policy_coverage(self, scenario: str, policies: List[Dict[str, Any]]) -> str:\n        \"\"\"Analyze how well current policies cover a given scenario\"\"\"\n        \n        if not policies:\n            return \"No policies available for analysis\"\n        \n        # Simple coverage analysis\n        coverage_areas = {\n            \"security\": 0,\n            \"access\": 0,\n            \"procedures\": 0,\n            \"compliance\": 0\n        }\n        \n        scenario_lower = scenario.lower()\n        \n        for policy in policies:\n            policy_content = (policy.get('content', '') + ' ' + policy.get('title', '')).lower()\n            \n            if any(term in policy_content for term in ['security', 'authentication', 'mfa']):\n                coverage_areas[\"security\"] += 1\n            if any(term in policy_content for term in ['access', 'permission', 'authorization']):\n                coverage_areas[\"access\"] += 1\n            if any(term in policy_content for term in ['procedure', 'process', 'workflow']):\n                coverage_areas[\"procedures\"] += 1\n            if any(term in policy_content for term in ['compliance', 'regulation', 'requirement']):\n                coverage_areas[\"compliance\"] += 1\n        \n        coverage_summary = f\"Security: {coverage_areas['security']} policies, Access: {coverage_areas['access']} policies, Procedures: {coverage_areas['procedures']} policies, Compliance: {coverage_areas['compliance']} policies\"\n        return coverage_summary\n    \n    def explain_policy_decision(\n        self,\n        decision: str,\n        reasoning: str,\n        citations: List[KGEnhancedPolicyCitation],\n        user_query: str\n    ) -> str:\n        \"\"\"Generate clear explanation for policy-based decisions\"\"\"\n        \n        # Prepare citation summaries\n        citation_summaries = []\n        for citation in citations[:3]:  # Top 3 citations\n            summary = f\"- {citation.document_title}: {citation.chunk_content[:100]}...\"\n            citation_summaries.append(summary)\n        \n        citations_text = \"\\n\".join(citation_summaries) if citation_summaries else \"No specific policies cited\"\n        \n        explanation_prompt = f\"\"\"Explain this IT support policy decision clearly:\n\nUser Request: \"{user_query}\"\nDecision: {decision}\nSystem Reasoning: {reasoning}\n\nSupporting Policies:\n{citations_text}\n\nProvide a clear, professional explanation that:\n1. States the decision clearly\n2. Explains the reasoning in simple terms\n3. References specific policy requirements\n4. Suggests next steps if applicable\n5. Addresses any user concerns\n\nKeep explanation concise but complete (max 200 words):\"\"\"\n        \n        explanation_result = local_llm_service.generate_response(\n            explanation_prompt,\n            max_tokens=400,\n            temperature=0.5,\n            context={\"task\": \"decision_explanation\"}\n        )\n        \n        return explanation_result.get(\"response\", reasoning)\n    \n    def smart_policy_search_with_feedback(\n        self,\n        query: str,\n        db: Session,\n        user_id: int,\n        feedback_data: Optional[Dict[str, Any]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Perform smart policy search with learning from user feedback\"\"\"\n        \n        # Get base search results\n        search_results = self.intelligent_policy_search(\n            query, db, user_id, k=5, use_conversation_context=True\n        )\n        \n        # If feedback is provided, use it to improve results\n        if feedback_data:\n            improved_results = self._apply_feedback_learning(\n                search_results, feedback_data, query\n            )\n            search_results[\"feedback_applied\"] = True\n            search_results[\"improved_results\"] = improved_results\n        \n        # Generate smart suggestions for follow-up\n        suggestions = self._generate_smart_suggestions(\n            query, search_results, db, user_id\n        )\n        search_results[\"smart_suggestions\"] = suggestions\n        \n        return search_results\n    \n    def _apply_feedback_learning(\n        self,\n        search_results: Dict[str, Any],\n        feedback: Dict[str, Any],\n        original_query: str\n    ) -> Dict[str, Any]:\n        \"\"\"Apply user feedback to improve search results\"\"\"\n        \n        # Analyze feedback using LLM\n        feedback_prompt = f\"\"\"Analyze user feedback to improve IT policy search results:\n\nOriginal Query: \"{original_query}\"\nUser Feedback: {feedback.get('feedback_text', 'No specific feedback')}\nRating: {feedback.get('rating', 'Not provided')}/5\nUseful Results: {feedback.get('useful_results', [])}\n\nSuggest improvements:\n1. Query refinements for better results\n2. Additional search terms to try\n3. Different search strategies\n4. Policy gaps identified\n\nReturn JSON:\n{{\n    \"suggested_query_refinements\": [\"refined query 1\", \"refined query 2\"],\n    \"additional_search_terms\": [\"term1\", \"term2\"],\n    \"search_strategy\": \"focused|broad|specific_domain\",\n    \"identified_gaps\": [\"gap1\", \"gap2\"]\n}}\n\nJSON Response:\"\"\"\n        \n        feedback_result = local_llm_service.generate_response(\n            feedback_prompt,\n            max_tokens=256,\n            temperature=0.4,\n            context={\"task\": \"feedback_analysis\"}\n        )\n        \n        try:\n            improvements = json.loads(feedback_result[\"response\"])\n            return improvements\n        except (json.JSONDecodeError, KeyError):\n            return {\"suggested_query_refinements\": [], \"error\": \"Feedback analysis failed\"}\n    \n    def _generate_smart_suggestions(\n        self,\n        query: str,\n        search_results: Dict[str, Any],\n        db: Session,\n        user_id: int\n    ) -> List[Dict[str, str]]:\n        \"\"\"Generate smart suggestions for follow-up queries or actions\"\"\"\n        \n        # Analyze search results to suggest improvements\n        citations = search_results.get(\"enhanced_citations\", [])\n        policy_analysis = search_results.get(\"policy_analysis\", {})\n        \n        if not citations:\n            return [{\n                \"type\": \"search_refinement\",\n                \"suggestion\": \"Try using more specific technical terms in your query\",\n                \"action\": \"refine_query\"\n            }]\n        \n        suggestions = []\n        \n        # Suggest related concepts from KG\n        graph_hops = search_results.get(\"graph_hops\", [])\n        if graph_hops:\n            related_concepts = [hop.to_concept for hop in graph_hops[:3]]\n            suggestions.append({\n                \"type\": \"related_concepts\",\n                \"suggestion\": f\"You might also want to explore: {', '.join(related_concepts)}\",\n                \"action\": \"explore_concepts\"\n            })\n        \n        # Suggest policy-based actions\n        if policy_analysis.get(\"recommended_action\"):\n            action = policy_analysis[\"recommended_action\"]\n            suggestions.append({\n                \"type\": \"recommended_action\",\n                \"suggestion\": f\"Based on policies, recommended action: {action}\",\n                \"action\": \"follow_recommendation\"\n            })\n        \n        # Suggest conversation context if available\n        user_patterns = search_results.get(\"conversation_context\", {}).get(\"user_patterns\", {})\n        if user_patterns.get(\"common_topics\"):\n            common_topics = user_patterns[\"common_topics\"][:2]\n            suggestions.append({\n                \"type\": \"related_topics\",\n                \"suggestion\": f\"Based on your history, you might also need help with: {', '.join(common_topics)}\",\n                \"action\": \"explore_history\"\n            })\n        \n        return suggestions[:4]  # Return top 4 suggestions\n    \n    def get_service_statistics(self, db: Session) -> Dict[str, Any]:\n        \"\"\"Get comprehensive statistics for the LLM-Enhanced Policy Service\"\"\"\n        \n        return {\n            \"llm_service\": local_llm_service.get_performance_stats(),\n            \"conversation_memory\": conversation_memory_service.get_memory_statistics(db),\n            \"prompt_engineering\": prompt_engineering_service.get_prompt_statistics(),\n            \"last_updated\": datetime.now().isoformat()\n        }\n\n\n# Global instance\nllm_enhanced_policy_service = LLMEnhancedPolicyService()\n"}}
</function_calls>

<function_calls>
<invoke name="mark_todo_as_done">
<parameter name="todo_ids">["d6ddc687-f723-472b-a834-46f716eaa601", "8965a9be-4f5c-4ca5-9528-ce4d954590fb", "3e62e397-4946-4f0b-8f14-44e9665c5fd8", "75ac0160-7f33-485b-af3c-1f127c803dcc"]
